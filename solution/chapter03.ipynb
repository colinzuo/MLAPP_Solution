{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.1 MLE for the Bernoulli/ binomial model\n",
    "Derive Equation 3.22 by optimizing the log of the likelihood in Equation 3.11.<br>\n",
    "$\\quad\\begin{aligned} p(D|θ) = θ^{N_1} (1 − θ)^{N_0} \\end{aligned}$ <span style=\"float:right\"> (3.11) </span><br>\n",
    "$\\quad\\begin{aligned} \\hatθ_{MLE} = \\frac{N_1}{N} \\end{aligned}$ <span style=\"float:right\"> (3.22) </span><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### answer\n",
    "---\n",
    "$log p(D|\\theta) = N_1log\\theta + N_0log(1-\\theta)$<br>\n",
    "$\\begin{aligned} \\frac{d\\ log p(D|\\theta)}{d\\theta} = N_1\\frac{1}{\\theta} - N_0\\frac{1}{1-\\theta} = 0 \\to \\hat\\theta_{MLE} = \\frac{N_1}{N_1+N_0} \\end{aligned}$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.2 Marginal likelihood for the Beta-Bernoulli model\n",
    "In Equation 5.23, we showed that the marginal likelihood is the ratio of the normalizing constants:\n",
    "$\\quad\\begin{aligned} p(D) = \\frac{Z(α_1 + N_1, α_0 + N_0)}{Z(α_1, α_0)} = \\frac{Γ(α_1 + N_1)Γ(α_0 + N_0)}{Γ(α_1 + α_0 + N)}\\frac{Γ(α_1 + α_0)}{Γ(α_1)Γ(α_0)} \\end{aligned}$ <span style=\"float:right\"> (3.80) </span><br>\n",
    "We will now derive an alternative derivation of this fact. By the chain rule of probability,<br>\n",
    "$\\quad p(x_{1:N} ) = p(x_1)p(x_2|x_1)p(x_3|x_{1:2})... $ <span style=\"float:right\"> (3.81) </span><br>\n",
    "In Section 3.3.4, we showed that the posterior predictive distribution is<br>\n",
    "$\\quad\\begin{aligned} p(X = k|D_{1:N} ) =\\frac{N_k + α_k}{\\sum_i N_i + α_i} = \\frac{N_k + α_k}{N + α} \\end{aligned}$ <span style=\"float:right\"> (3.82) </span><br>\n",
    "where k ∈ {0, 1} and $D_{1:N}$ is the data seen so far. Now suppose D = H, T, T, H, H or D = 1, 0, 0, 1, 1.\n",
    "Then<br>\n",
    "$\\quad\\begin{aligned} p(D) = \\frac{α_1}{α}\\frac{α_0}{α + 1}\\frac{α_0 + 1}{α + 2}\\frac{α_1 + 1}{α + 3}\\frac{α_1 + 2}{α + 4} \\end{aligned}$ <span style=\"float:right\"> (3.83) </span><br>\n",
    "$\\quad\\begin{aligned} = \\frac{[α_1(α_1 + 1)(α_1 + 2)] [α_0(α_0 + 1)]}{α(α + 1)···(α + 4)} \\end{aligned}$ <span style=\"float:right\"> (3.84) </span><br>\n",
    "$\\quad\\begin{aligned} = \\frac{[(α_1)···(α_1 + N_1 − 1)] [(α_0)···(α_0 + N_0 − 1)]}{(α)···(α + N − 1)} \\end{aligned}$ <span style=\"float:right\"> (3.85) </span><br>\n",
    "Show how this reduces to Equation 3.80 by using the fact that, for integers, (α − 1)! = Γ(α)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### answer\n",
    "---\n",
    "$\\quad\\begin{aligned} (3.85) \\to \\frac{\\Gamma(a_1 + N_1)}{\\Gamma(a_1)}\\frac{\\Gamma(a_0 + N_0)}{\\Gamma(a_0)}\\frac{\\Gamma(a_1 + a_0)}{\\Gamma(a_1 + a_0 + N)} \\to (3.80)  \\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.3 Posterior predictive for Beta-Binomial model\n",
    "Recall from Equation 3.34 that the posterior predictive for the Beta-Binomial is given by<br>\n",
    "$\\quad\\begin{aligned} p(x|n, D) = Bb(x|α_0', α_1', n) \\end{aligned}$ <span style=\"float:right\"> (3.86) </span><br>\n",
    "$\\quad\\begin{aligned} = \\frac{B(x + α_1', n − x + α_0')}{B(α_1', α_0')}{n \\choose x}\\end{aligned}$ <span style=\"float:right\"> (3.87) </span><br>\n",
    "Prove that this reduces to<br>\n",
    "$\\quad\\begin{aligned} p(\\tilde{x} = 1|D) = \\frac{α_1'}{α_0' + α_1'}\\end{aligned}$ <span style=\"float:right\"> (3.88) </span><br>\n",
    "when n = 1 (and hence x ∈ {0, 1}). i.e., show that<br>\n",
    "$\\quad\\begin{aligned} Bb(1|α_1', α_0', 1) = \\frac{α_1'}{α_0' + α_1'}\\end{aligned}$ <span style=\"float:right\"> (3.85) </span><br>\n",
    "Hint: use the fact that<br>\n",
    "$\\quad\\begin{aligned} Γ(α_0 + α_1 + 1) = (α_0 + α_1)Γ(α_0 + α_1) \\end{aligned}$ <span style=\"float:right\"> (3.90) </span><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "$\\quad \\Gamma(a) = (a-1)!$<br>\n",
    "$\\quad\\begin{aligned} B(a, b) = \\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a + b)} \\end{aligned}$ <span style=\"float:right\"> (2.61) </span><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### answer\n",
    "---\n",
    "$\\begin{aligned} Bb(1|a'_1, a'_0, 1) = \\frac{B(1+a'_1, a'_0)}{B(a'_1, a'_0)} = \\frac{\\Gamma(1+a'_1)\\Gamma(a'_0)}{\\Gamma(1+a'_1+a'_0)}\\frac{\\Gamma(a'_1+a'_0)}{\\Gamma(a'_1)\\Gamma(a'_0)} \\to \\frac{a'_1}{a'_0+a'_1} \\quad (3.85) \\end{aligned}$<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.4 Beta updating from censored likelihood\n",
    "(Source: Gelman.) Suppose we toss a coin n = 5 times. Let X be the number of heads. We observe that\n",
    "there are fewer than 3 heads, but we don’t know exactly how many. Let the prior probability of heads be\n",
    "p(θ) = Beta(θ|1, 1). Compute the posterior p(θ|X < 3) up to normalization constants, i.e., derive an\n",
    "expression proportional to p(θ, X < 3). Hint: the answer is a mixture distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "$\\quad\\begin{aligned} Bin(k|n, \\theta) = {n \\choose k} \\theta^k(1-\\theta)^{n-k} \\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### answer\n",
    "---\n",
    "$\\begin{aligned} p(\\theta|X < 3) = \\frac{p(\\theta, X < 3)}{p(X < 3)} \\propto p(\\theta, X < 3) = p(X < 3|\\theta)p(\\theta) \\end{aligned}$<br>\n",
    "$\\begin{aligned} = Bin(0|5, \\theta) + Bin(1|5, \\theta) + Bin(2|5, \\theta) = (1-\\theta)^5 + 5\\theta(1-\\theta)^4 + 10\\theta^2(1-\\theta)^3 \\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.5 Uninformative prior for log-odds ratio\n",
    "Let<br>\n",
    "$\\quad\\begin{aligned} \\phi = logit(θ) = log \\frac{θ}{1 − θ} \\end{aligned}$ <span style=\"float:right\"> (3.91) </span><br>\n",
    "Show that if $p(\\phi) ∝ 1$, then p(θ) ∝ Beta(θ|0, 0). Hint: use the change of variables formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### answer\n",
    "---\n",
    "$\\begin{aligned} p(\\theta) = p(\\phi)\\frac{d\\phi}{d\\theta} = p(\\phi)\\frac{1}{\\theta(1-\\theta)} \\propto \\frac{1}{\\theta(1-\\theta)} = Beta(\\theta|0, 0) \\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.6 MLE for the Poisson distribution\n",
    "The Poisson pmf is defined as $Poi(x|λ) = e^{−\\lambda}\\frac{\\lambda^x}{x!}$ , for x ∈ {0, 1, 2,...} where λ > 0 is the rate\n",
    "parameter. Derive the MLE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### answer\n",
    "---\n",
    "$\\begin{aligned} log Poi(D|\\lambda) = log (e^{-\\lambda}\\frac{\\lambda^{x_1}}{x_1!}) + \\cdots + log (e^{-\\lambda}\\frac{\\lambda^{x_n}}{x_n!} ) = -\\lambda N + \\sum^N_{i=1}x_ilog\\lambda - \\sum^N_{i=1}logx_i!\\end{aligned}$<br>\n",
    "$\\begin{aligned} \\frac{d log Poi(D|\\lambda)}{d\\lambda} = -N + \\frac{\\sum^N_{i=1}x_i}{\\lambda} = 0 \\to \\hat\\lambda = \\frac{\\sum^N_{i=1}x_i}{N} \\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.7 Bayesian analysis of the Poisson distribution\n",
    "In Exercise 3.6, we defined the Poisson distribution with rate λ and derived its MLE. Here we perform a\n",
    "conjugate Bayesian analysis.<br>\n",
    "a. Derive the posterior $p(\\lambda|D)$ assuming a conjugate prior $p(\\lambda) = Ga(\\lambda|a, b) ∝ \\lambda^{a−1}e^{−\\lambda b}$. Hint: the\n",
    "posterior is also a Gamma distribution.<br>\n",
    "b. What does the posterior mean tend to as a → 0 and b → 0? (Recall that the mean of a Ga(a, b)\n",
    "distribution is a/b.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### answer\n",
    "---\n",
    "### a\n",
    "$\\begin{aligned} p(\\lambda|D) \\propto e^{-N\\lambda}\\lambda^{\\sum_i x_i}\\lambda^{a-1}e^{-\\lambda b} = \\lambda^{\\sum_i x_i + a - 1}e^{-\\lambda (N+b)} \\to Ga(\\lambda|\\sum_i x_i + a, N + b) \\end{aligned}$\n",
    "### b\n",
    "$\\begin{aligned} E[\\lambda] = \\frac{\\sum_i x_i + a}{N + b} \\quad (a \\to 0, b \\to 0) = \\frac{\\sum_i x_i}{N}  \\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.8 MLE for the uniform distribution\n",
    "(Source: Kaelbling.) Consider a uniform distribution centered on 0 with width 2a. The density function is\n",
    "given by<br>\n",
    "$\\quad\\begin{aligned} p(x) = \\frac{1}{2a}I(x ∈ [−a, a]) \\end{aligned}$ <span style=\"float:right\"> (3.92) </span><br>\n",
    "a. Given a data set $x_1,...,x_n$, what is the maximum likelihood estimate of a (call it $\\hat{a}$)?<br>\n",
    "b. What probability would the model assign to a new data point $x_{n+1}$ using $\\hat{a}$?<br>\n",
    "c. Do you see any problem with the above approach? Briefly suggest (in words) a better approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### answer\n",
    "---\n",
    "### a\n",
    "$\\begin{aligned} p(D|a) = (\\frac{1}{2a})^N \\quad if \\quad a >= max_i |x_i| \\end{aligned}$<br>\n",
    "$\\begin{aligned} p(D|a) = 0 \\qquad otherwise \\end{aligned}$<br>\n",
    "$\\begin{aligned} \\hat{a} = max_i |x_i| \\end{aligned}$\n",
    "### b\n",
    "$\\begin{aligned} p(x_{n+1}|\\hat{a}) = \\frac{1}{2\\hat{a}} \\quad if \\quad |x_{n+1}| <= \\hat{a} \\end{aligned}$<br>\n",
    "$\\begin{aligned} p(x_{n+1}|\\hat{a}) = 0 \\qquad otherwise \\end{aligned}$<br>\n",
    "### c\n",
    "it puts zero probability mass outside the training data, add a margin inverse proportional to the data size N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.9 Bayesian analysis of the uniform distribution\n",
    "Consider the uniform distribution Unif(0, θ). The maximum likelihood estimate is $\\hat{θ} = max(D)$, as we\n",
    "saw in Exercise 3.8, but this is unsuitable for predicting future data since it puts zero probability mass\n",
    "outside the training data. In this exercise, we will perform a Bayesian analysis of the uniform distribution\n",
    "(following (Minka 2001a)). The conjugate prior is the Pareto distribution, p(θ) = Pareto(θ|b, K), defined in\n",
    "Section 2.4.6. Given a Pareto prior, the joint distribution of θ and D = $(x_1,...,x_N )$ is<br>\n",
    "$\\quad\\begin{aligned} p(D, θ) = \\frac{Kb^K}{θ^{N+K+1}} I(θ ≥ max(D) \\cap θ ≥ b) \\end{aligned}$ <span style=\"float:right\"> (3.93) </span><br>\n",
    "Let m = max(D). The evidence (the probability that all N samples came from the same uniform\n",
    "distribution) is\n",
    "$\\quad\\begin{aligned} p(D) = \\int^{∞}_{max(m, b)} \\frac{Kb^K}{θ^{N+K+1}} dθ \\end{aligned}$ <span style=\"float:right\"> (3.94) </span><br>\n",
    "$\\quad\\begin{aligned} = \\bigg\\{ \\begin{array}{l} \\frac{K}{(N+K)b^N} \\qquad if \\quad m ≤ b \\\\ \\frac{Kb^K}{(N+K)m^{N+K}} \\qquad if \\quad m>b \\end{array} \\end{aligned}$ <span style=\"float:right\"> (3.95) </span><br>\n",
    "Derive the posterior p(θ|D), and show that if can be expressed as a Pareto distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### answer\n",
    "---\n",
    "$\\quad\\begin{aligned} Pareto(x|k, m) = km^kx^{-(k+1)}I(x >= m) \\end{aligned}$ <span style=\"float:right\"> (2.63) </span><br>\n",
    "$\\begin{aligned} p(\\theta|D) = \\frac{p(D, \\theta)}{p(D)} \\end{aligned}$\n",
    "$\\quad\\begin{aligned} = I(\\theta >= m \\cap \\theta >= b) \\bigg\\{ \\begin{array}{l} \\frac{(N+K)b^{N+K}}{\\theta^{N+K+1}} \\qquad if \\quad m ≤ b \\\\ \\frac{(N+K)m^{N+K}}{\\theta^{N+K+1}} \\qquad if \\quad m>b \\end{array} \\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.10 Taxicab (tramcar) problem\n",
    "Suppose you arrive in a new city and see a taxi numbered 100. How many taxis are there in this city? Let\n",
    "us assume taxis are numbered sequentially as integers starting from 0, up to some unknown upper bound\n",
    "θ. (We number taxis from 0 for simplicity; we can also count from 1 without changing the analysis.) Hence\n",
    "the likelihood function is p(x) = U(0, θ), the uniform distribution. The goal is to estimate θ. We will use\n",
    "the Bayesian analysis from Exercise 3.9.<br>\n",
    "a. Suppose we see one taxi numbered 100, so D = {100}, m = 100, N = 1. Using an (improper)\n",
    "non-informative prior on θ of the form $p(θ) = P_a(θ|0, 0) ∝ 1/θ$, what is the posterior p(θ|D)?<br>\n",
    "b. Compute the posterior mean, mode and median number of taxis in the city, if such quantities exist.<br>\n",
    "c. Rather than trying to compute a point estimate of the number of taxis, we can compute the predictive\n",
    "density over the next taxicab number using<br>\n",
    "$\\quad\\begin{aligned} p(D'|D, α) = \\int p(D'|θ)p(θ|D, α)dθ = p(D'|β) \\end{aligned}$ <span style=\"float:right\"> (3.96) </span><br>\n",
    "where α = (b, K) are the hyper-parameters, β = (c, N + K) are the updated hyper-parameters. Now\n",
    "consider the case D = {m}, and $D' = {x}$. Using Equation 3.95, write down an expression for<br>\n",
    "$\\quad\\begin{aligned} p(x|D, α) \\end{aligned}$ <span style=\"float:right\"> (3.97) </span><br>\n",
    "As above, use a non-informative prior b = K = 0.<br>\n",
    "d. Use the predictive density formula to compute the probability that the next taxi you will see (say,\n",
    "the next day) has number 100, 50 or 150, i.e., compute p(x = 100|D, α), p(x = 50|D, α), p(x =\n",
    "150|D, α).<br>\n",
    "e. Briefly describe (1-2 sentences) some ways we might make the model more accurate at prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### answer\n",
    "---\n",
    "### a\n",
    "$\\begin{aligned} p(\\theta|D) = I(\\theta > 100)\\frac{(N+K)m^{N+K}}{\\theta^{N+K+1}} (N=1, K=0, m=100) = \\frac{100}{\\theta^2}I(\\theta > 100) \\end{aligned}$\n",
    "### b\n",
    "$\\begin{aligned} E[\\theta|D] = \\int^{\\infty}{100} \\theta \\frac{100}{\\theta^2} d\\theta = 100 * (log\\infty - log100) \\to not \\quad exist \\end{aligned}$<br>\n",
    "$\\begin{aligned} p(\\theta|D) = \\frac{100}{\\theta^2}I(\\theta > 100) \\to \\hat\\theta = 100 \\end{aligned}$\n",
    "$\\begin{aligned} \\int^{median}{100} \\frac{100}{\\theta^2} d\\theta = 0.5 \\to median = 200 \\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c\n",
    "$\\begin{aligned} p(D'|β) = \\int^{\\infty}_{c} \\frac{1}{\\theta}I(\\theta > D') \\frac{(N+K)c^{N+K}}{\\theta^{N+K+1}}d\\theta = \\bigg\\{ \\begin{array}{l} \\frac{N+K}{(N+K+1)c} \\qquad if \\quad D' ≤ c \\\\ \\frac{(N+K)c^{N+K}}{(N+K+1)D'^{N+K+1}} \\qquad if \\quad D' > c \\end{array} \\end{aligned}$<br>\n",
    "$\\begin{aligned} p(x|D, α) = \\bigg\\{ \\begin{array}{l} \\frac{1}{2m} \\qquad if \\quad x ≤ m \\\\ \\frac{m}{{2x^2}} \\qquad if \\quad x > m \\end{array} \\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d\n",
    "$\\begin{aligned} p(x = 100|D, α) = \\frac{1}{200} \\end{aligned}$<br>\n",
    "$\\begin{aligned} p(x = 50|D, α) = \\frac{1}{200} \\end{aligned}$<br>\n",
    "$\\begin{aligned} p(x = 150|D, α) = \\frac{1}{450} \\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e\n",
    "use informative prior prameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.11 Bayesian analysis of the exponential distribution\n",
    "A lifetime X of a machine is modeled by an exponential distribution with unknown parameter θ. The\n",
    "likelihood is $p(x|θ) = θe^{−θx}$ for x ≥ 0, θ > 0.<br>\n",
    "a. Show that the MLE is $\\hat{θ} = 1/\\bar{x}$, where $x = \\frac{1}{N}\\sum^{N}_{i=1} x_i$.<br>\n",
    "b. Suppose we observe $X_1 = 5, X_2 = 6, X_3 = 4$ (the lifetimes (in years) of 3 different iid machines).\n",
    "What is the MLE given this data?<br>\n",
    "c. Assume that an expert believes θ should have a prior distribution that is also exponential<br>\n",
    "$\\quad\\begin{aligned} p(θ) = Expon(θ|λ) \\end{aligned}$ <span style=\"float:right\"> (3.98) </span><br>\n",
    "Choose the prior parameter, call it $\\hat\\lambda$, such that E [θ]=1/3. Hint: recall that the Gamma distribution\n",
    "has the form<br>\n",
    "$\\quad\\begin{aligned} Ga(θ|a, b) ∝ θ^{a−1}e^{−θb} \\end{aligned}$ <span style=\"float:right\"> (3.99) </span><br>\n",
    "and its mean is a/b.<br>\n",
    "d. What is the posterior, $p(θ|D, \\hat\\lambda)$?<br>\n",
    "e. Is the exponential prior conjugate to the exponential likelihood?<br>\n",
    "f. What is the posterior mean, $E[θ|D, \\hat\\lambda]$?<br>\n",
    "g. Explain why the MLE and posterior mean differ. Which is more reasonable in this example?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### answer\n",
    "---\n",
    "### a\n",
    "$\\begin{aligned} log p(D|\\theta) = Nlog\\theta - \\theta\\sum_i x_i \\end{aligned}$<br>\n",
    "$\\begin{aligned} log p(D|\\theta)' = 0 \\to \\frac{N}{\\hat\\theta} = \\sum_i x_i \\to \\hat\\theta = \\frac{N}{\\sum_i x_i} \\end{aligned}$\n",
    "### b\n",
    "$\\begin{aligned} p(x|\\hat\\theta) = 1/5e^{-1/5x} \\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c\n",
    "$\\begin{aligned} Ga(T|shape=a, rate=b) = \\frac{b^a}{\\Gamma(a)}T^{a-1}e^{-Tb} \\end{aligned}$ <span style=\"float:right\"> (2.55) </span><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{aligned} Expon(\\theta|\\lambda) = Ga(T|a = 1, \\lambda) = \\lambda e^{-\\theta \\lambda} \\end{aligned}$\n",
    "$\\begin{aligned} E[\\theta] = 1/3 \\to \\lambda = 3 \\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d\n",
    "$\\begin{aligned} p(\\theta|D, \\lambda) = \\frac{p(D|\\theta, \\lambda)p(\\theta|\\lambda)}{p(D|\\lambda)} \\end{aligned}$<br>\n",
    "$\\begin{aligned} p(D|\\theta, \\lambda)p(\\theta|\\lambda) = \\theta^N e^{-\\theta \\sum_i x_i} \\lambda e^{-\\theta\\lambda} = \\lambda \\theta^N e^{-\\theta (\\sum_i x_i + \\lambda)}\\end{aligned}$<br>\n",
    "$\\begin{aligned} p(D|\\lambda) = \\int^\\infty_0 \\lambda \\theta^N e^{-\\theta (\\sum_i x_i + \\lambda)} d\\theta = \\lambda \\frac{\\Gamma(N+1)}{(\\sum_i x_i + \\lambda)^{N+1}} \\end{aligned}$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{aligned} p(\\theta|D, \\lambda) = \\frac{\\theta^N e^{-\\theta(\\sum_i x_i + \\lambda)}(\\sum_i x_i + \\lambda)^{N+1}}{\\Gamma(N+1)}  = Ga(\\theta|a=N+1, b=\\sum_i x_i + \\lambda)\\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e\n",
    "yes\n",
    "### f\n",
    "$\\begin{aligned} E[\\theta|D, \\lambda] = \\frac{N+1}{\\sum_i x_i + \\lambda} \\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g\n",
    "MLE didn't consider prior, Posterior is more reasonable as the prior is informative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.12 MAP estimation for the Bernoulli with non-conjugate priors\n",
    "(Source: Jaakkola.) In the book, we discussed Bayesian inference of a Bernoulli rate parameter with the\n",
    "prior p(θ) = Beta(θ|α, β). We know that, with this prior, the MAP estimate is given by\n",
    "$\\quad\\begin{aligned} \\hat\\theta = \\frac{N_1 + α − 1}{N + α + β − 2} \\end{aligned}$ <span style=\"float:right\"> (3.100) </span><br>\n",
    "where $N_1$ is the number of heads, $N_0$ is the number of tails, and $N = N_0 + N_1$ is the total number of\n",
    "trials.<br>\n",
    "a. Now consider the following prior, that believes the coin is fair, or is slightly biased towards tails:<br>\n",
    "$\\quad\\begin{aligned} p(θ) = \\left\\{ \\begin{array}{l} 0.5 \\qquad if θ = 0.5 \\\\ 0.5 \\qquad if θ = 0.4 \\\\ 0  \\qquad otherwise \\end{array}\n",
    "\\right. \\end{aligned}$ <span style=\"float:right\"> (3.101) </span><br>\n",
    "Derive the MAP estimate under this prior as a function of $N_1$ and N.<br>\n",
    "b. Suppose the true parameter is θ = 0.41. Which prior leads to a better estimate when N is small?\n",
    "Which prior leads to a better estimate when N is large?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### answer\n",
    "---\n",
    "### a\n",
    "$\\begin{aligned} p(\\theta|D) = \\frac{p(D|\\theta)p(\\theta)}{p(D)} = \\left\\{ \\begin{array}{l} \\frac{0.5^N}{0.5^N + 0.4^{N_1}0.6^{N - N_1}} \\qquad if \\quad θ = 0.5 \\\\ \\frac{0.4^{N_1}0.6^{N - N_1}}{0.5^N + 0.4^{N_1}0.6^{N - N_1}} \\qquad if \\quad θ = 0.4 \\\\ 0  \\qquad otherwise \\end{array}\n",
    "\\right. \\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{aligned} \\frac{p(\\theta = 0.5|D)}{p(\\theta = 0.4|D)} = \\frac{0.5^N}{0.4^{N_1}0.6^{N-N_1}}  \\end{aligned}$\n",
    "$\\begin{aligned} log \\frac{p(\\theta = 0.5|D)}{p(\\theta = 0.4|D)} = N_1 log3/2 - N log6/5 < 0 \\to \\frac{N_1}{N} < \\frac{log6/5}{log3/2} \\to \\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{aligned} \\theta_{MAP} = \\frac{p(D|\\theta)p(\\theta)}{p(D)} = \\left\\{ \\begin{array}{l} 0.4 \\qquad if \\quad \\frac{N_1}{N} < \\frac{log6/5}{log3/2} \\sim 0.45 \\\\ 0.5 \\qquad otherwise \\end{array}\n",
    "\\right. \\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b\n",
    "when N is small, prior described in a is better as Beta prior will probably overfit<br>\n",
    "when N is large, Beta prior should be better as it should be more close to the real value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.13 Posterior predictive distribution for a batch of data with the dirichlet-multinomial model\n",
    "In Equation 3.51, we gave the posterior predictive distribution for a single multinomial trial using a\n",
    "dirichlet prior. Now consider predicting a batch of new data, $\\tilde{D} = (X_1,...,X_m)$, consisting of m single\n",
    "multinomial trials (think of predicting the next m words in a sentence, assuming they are drawn iid).\n",
    "Derive an expression for<br>\n",
    "$\\quad\\begin{aligned} p(\\tilde{D}|D, α) \\end{aligned}$ <span style=\"float:right\"> (3.102) </span><br>\n",
    "Your answer should be a function of α, and the old and new counts (sufficient statistics), defined as<br>\n",
    "$\\quad\\begin{aligned} N^{old}_k = \\sum_{i∈D}I(x_i = k) \\end{aligned}$ <span style=\"float:right\"> (3.103) </span><br>\n",
    "$\\quad\\begin{aligned} N^{new}_k = \\sum_{i∈\\tilde{D}}I(x_i = k) \\end{aligned}$ <span style=\"float:right\"> (3.104) </span><br>\n",
    "Hint: recall that, for a vector of counts, $N_{1:K}$, the marginal likelihood (evidence) is given by<br>\n",
    "$\\quad\\begin{aligned} p(D|α) = \\frac{Γ(α)}{Γ(N + α)}\\prod_{k}\\frac{Γ(N_k + α_k)}{Γ(α_k)} \\end{aligned}$ <span style=\"float:right\"> (3.105) </span><br>\n",
    "where $α = \\sum_k α_k$ and $N =\\sum_k N_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### answer\n",
    "---\n",
    "$\\begin{aligned} B(a) = \\frac{\\prod^K_{k=1} \\Gamma(a_k)}{\\Gamma(a_0)} \\end{aligned}$ <span style=\"float:right\"> (2.76) </span><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{aligned} p(\\tilde{D}|D, α) = \\frac{B(a_{new})}{B(a_{old} \\quad a)} = \\frac{\\Gamma(N^{old} + a)}{\\Gamma(N^{old} + a + N^{new})} \\prod^K_{k=1}\\frac{ \\Gamma(N^{new}_k + N^{old}_k + a_k)}{\\Gamma(N^{old}_k + a_k)} \\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.14 Posterior predictive for Dirichlet-multinomial\n",
    "(Source: Koller.).<br>\n",
    "a. Suppose we compute the empirical distribution over letters of the Roman alphabet plus the space\n",
    "character (a distribution over 27 values) from 2000 samples. Suppose we see the letter “e” 260 times.\n",
    "What is $p(x_{2001} = e|D)$, if we assume $θ ∼ Dir(α_1,...,α_{27})$, where $α_k = 10$ for all k?<br>\n",
    "b. Suppose, in the 2000 samples, we saw “e” 260 times, “a” 100 times, and “p” 87 times. What is\n",
    "$p(x_{2001} = p, x_{2002} = a|D)$, if we assume $θ ∼ Dir(α_1,...,α_{27})$, where $α_k = 10$ for all k? Show\n",
    "your work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### answer\n",
    "---\n",
    "### a\n",
    "$\\begin{aligned} p(x_{2001} = e|D) = \\frac{270}{2270} \\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b\n",
    "$\\begin{aligned} p(x_{2001} = p, x_{2002} = a|D) = \\frac{1}{2271 \\times 2270} 97 \\times 110 \\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.15 Setting the beta hyper-parameters\n",
    "Suppose $θ ∼ β(α_1, α_2)$ and we believe that E [θ] = m and var [θ] = v. Using Equation 2.62, solve for\n",
    "$α_1$ and $α_2$ in terms of m and v. What values do you get if m = 0.7 and v = $0.2^2$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### answer\n",
    "----\n",
    "$\\quad\\begin{aligned} mean = \\frac{a}{a + b} \\quad mode = \\frac{a - 1}{a + b -2} \\quad var = \\frac{ab}{(a + b)^2(a + b +1)} \\end{aligned}$ <span style=\"float:right\"> (2.62) </span><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{aligned} m = \\frac{a}{a + b} \\to b = \\frac{1-m}{m}a \\end{aligned}$<br>\n",
    "$\\begin{aligned} v = \\frac{ab}{(a+b)^2(a+b+1)} \\to a = \\frac{m \\times (m(1 - m) - v)}{v} \\end{aligned}$<br>\n",
    "$\\begin{aligned} b = \\frac{(1 - m) \\times (m(1 - m) - v)}{v} \\end{aligned}$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{aligned} m = 0.7 \\quad v = 0.2^2 \\to a = 2.975 \\quad b = 1.275 \\end{aligned}$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.16 Setting the beta hyper-parameters II\n",
    "(Source: Draper.) Suppose $θ ∼ β(α_1, α_2)$ and we believe that $E [θ] = m$ and $p(l < θ < u)=0.95$.<br>\n",
    "Write a program that can solve for $α_1$ and $α_2$ in terms of m, l and u. Hint: write $α_2$ as a function of $α_1$\n",
    "and m, so the pdf only has one unknown; then write down the probability mass contained in the interval\n",
    "as an integral, and minimize its squared discrepancy from 0.95. What values do you get if m = 0.15,\n",
    "l = 0.05 and u = 0.3? What is the equivalent sample size of this prior?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### answer\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import beta\n",
    "from scipy.integrate import quad\n",
    "\n",
    "def loss(a1, m, l, u):\n",
    "    b = beta(a1, (1/m - 1)*a1)\n",
    "    pdf = lambda x : 1/b*x**(a1-1)*(1-x)**((1/m-1)*a1-1)\n",
    "    return (quad(pdf, l, u)[0] - 0.95)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      fun: 3.6913317291686702e-09\n",
       " hess_inv: <1x1 LbfgsInvHessProduct with dtype=float64>\n",
       "      jac: array([ -3.19171893e-06])\n",
       "  message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'\n",
       "     nfev: 26\n",
       "      nit: 12\n",
       "   status: 0\n",
       "  success: True\n",
       "        x: array([ 4.50374794])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "m = 0.15\n",
    "l = 0.05\n",
    "u = 0.3\n",
    "\n",
    "res = minimize(loss, 0.5, args=(m, l, u), bounds=[(0,None)])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a1 = [ 4.50374794], a2 = [ 25.52123834]\n"
     ]
    }
   ],
   "source": [
    "a1 = res.x\n",
    "a2 = (1/m - 1)*a1\n",
    "print(\"a1 = {}, a2 = {}\".format(a1, a2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.17 Marginal likelihood for beta-binomial under uniform prior\n",
    "Suppose we toss a coin N times and observe $N_1$ heads. Let $N_1 ∼ Bin(N,θ)$ and θ ∼ Beta(1, 1). Show\n",
    "that the marginal likelihood is $p(N_1|N)=\\frac{1}{N + 1}$. Hint: Γ(x + 1) = x! if x is an integer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### answer\n",
    "---\n",
    "$\\begin{aligned} p(N_1|N) = \\int^1_0 {N \\choose N_1} \\theta^{N_1}(1-\\theta)^{N - N_1} d\\theta = {N \\choose N_1} B(N_1 + 1, N - N_1 + 1) \\end{aligned}$<br>\n",
    "$\\begin{aligned} = \\frac{N!}{N_1!(N-N_1)!}\\frac{\\Gamma(N_1 + 1)\\Gamma(N - N_1 + 1)}{\\Gamma(N + 2)} \\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{aligned} = \\frac{1}{N + 1} \\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.18 Bayes factor for coin tossing\n",
    "Suppose we toss a coin N = 10 times and observe $N_1 = 9$ heads. Let the null hypothesis be that the\n",
    "coin is fair, and the alternative be that the coin can have any bias, so $p(θ) = Unif(0, 1)$. Derive the\n",
    "Bayes factor $BF_{1,0}$ in favor of the biased coin hypothesis. What if N = 100 and $N_1 = 90$? Hint: see\n",
    "Exercise 3.17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### answer\n",
    "---\n",
    "$\\quad\\begin{aligned} BF_{1,0} = \\frac{p(D|M_1)}{p(D|M_0)} = \\frac{p(M_1|D)}{p(M_0|D)} / \\frac{p(M_1)}{p(M_0)} \\end{aligned}$ <span style=\"float:right\"> (5.39) </span><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{aligned} p(D|M_1) = 1/(10 + 1) \\end{aligned}$<br>\n",
    "$\\begin{aligned} p(D|M_0) = 10 \\times 0.5^{10} = 0.009765625 \\end{aligned}$<br>\n",
    "$\\begin{aligned} BF_{1,0} = 9.3 \\end{aligned}$<br>\n",
    "if $N = 100$, $N_1 = 90$, then<br>\n",
    "$\\begin{aligned} p(D|M_1) = 1/(100 + 1) \\end{aligned}$<br>\n",
    "$\\begin{aligned} p(D|M_0) = {100 \\choose 90} \\times 0.5^{100} = 1.3655 e^{-17} \\end{aligned}$<br>\n",
    "$\\begin{aligned} BF_{1,0} = 7.25 e^{14} \\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.19 Irrelevant features with naive Bayes\n",
    "(Source: Jaakkola.) Let $x_{iw} = 1$ if word w occurs in document i and $x_{iw} = 0$ otherwise. Let $θ_{cw}$ be the\n",
    "estimated probability that word w occurs in documents of class c. Then the log-likelihood that document\n",
    "x belongs to class c is<br>\n",
    "$\\quad\\begin{aligned} log p(x_i|c, θ) = log \\prod^{W}_{w=1}θ^{x_{iw}}_{cw} (1 − θ_{cw})^{1−x_{iw}} \\end{aligned}$ <span style=\"float:right\"> (3.106) </span><br>\n",
    "$\\quad\\begin{aligned} = \\sum^{W}_{w=1}x_{iw} logθ_{cw} + (1 − x_{iw})log(1 − θ_{cw}) \\end{aligned}$ <span style=\"float:right\"> (3.107) </span><br>\n",
    "$\\quad\\begin{aligned} = \\sum^{W}_{w=1}x_{iw} log \\frac{θ_{cw}}{1 − θ_{cw}} + \\sum_wlog(1 − θ_{cw}) \\end{aligned}$ <span style=\"float:right\"> (3.108) </span><br>\n",
    "where W is the number of words in the vocabulary. We can write this more succintly as\n",
    "$\\quad\\begin{aligned} log p(x_i|c, θ) = \\phi(x_i)^T \\beta_c \\end{aligned}$ <span style=\"float:right\"> (3.109) </span><br>\n",
    "where $x_i = (x_{i1},...,x_{iW} )$ is a bit vector, $φ(x_i)=(x_i, 1)$, and<br>\n",
    "$\\quad\\begin{aligned} β_c = (log \\frac{θ_{c1}}{1 − θ_{c1}},..., log \\frac{θ_{cW}}{1 − θ_{cW}}, \\sum_wlog(1 − θ_{cw}))^T \\end{aligned}$ <span style=\"float:right\"> (3.110) </span><br>\n",
    "We see that this is a linear classifier, since the class-conditional density is a linear function (an inner\n",
    "product) of the parameters $\\beta_c$.<br>\n",
    "a. Assuming p(C = 1) = p(C = 2) = 0.5, write down an expression for the log posterior odds ratio,\n",
    "$log_2\\frac{p(c=1|x_i)}{p(c=2|x_i)}$ , in terms of the features $\\phi(x_i)$ and the parameters $\\beta_1$ and $\\beta_2$.<br>\n",
    "b. Intuitively, words that occur in both classes are not very “discriminative”, and therefore should not\n",
    "affect our beliefs about the class label. Consider a particular word w. State the conditions on $θ_{1,w}$ and\n",
    "$θ_{2,w}$ (or equivalently the conditions on $β_{1,w}$, $β_{2,w}$) under which the presence or absence of w in a\n",
    "test document will have no effect on the class posterior (such a word will be ignored by the classifier).\n",
    "Hint: using your previous result, figure out when the posterior odds ratio is 0.5/0.5.<br>\n",
    "c. The posterior mean estimate of θ, using a Beta(1,1) prior, is given by\n",
    "$\\quad\\begin{aligned} \\hat{θ}_{cw} = \\frac{1 + \\sum_{i∈c} x_{iw}}{2 + n_c}\\end{aligned}$ <span style=\"float:right\"> (3.111) </span><br>\n",
    "where the sum is over the $n_c$ documents in class c. Consider a particular word w, and suppose it\n",
    "always occurs in every document (regardless of class). Let there be $n_1$ documents of class 1 and $n_2$ be\n",
    "the number of documents in class 2, where $n_1 \\ne n_2$ (since e.g., we get much more non-spam than\n",
    "spam; this is an example of class imbalance). If we use the above estimate for $θ_{cw}$, will word w be\n",
    "ignored by our classifier? Explain why or why not.<br>\n",
    "d. What other ways can you think of which encourage “irrelevant” words to be ignored?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### answer\n",
    "---\n",
    "### a\n",
    "$\\begin{aligned} log_2\\frac{p(c=1|x_i)}{p(c=2|x_i)} = log_2\\frac{p(x_i|c=1)p(c=1)}{p(x_i|c=2)p(c=2)} = \\phi(x_i)^T (\\beta_1 - \\beta_2) \\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b\n",
    "if no effect, then $\\beta_{1,w} = \\beta_{2,w}$, then $\\theta_{1,w} = \\theta_{2,w}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c\n",
    "not ignored because<br>\n",
    "$\\begin{aligned} n_1 \\ne n_2 \\to \\theta_{1w} = \\frac{1 + n_1}{2 + n_1} \\ne \\theta_{2w} = \\frac{1 + n_2}{2 + n_2} \\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d\n",
    "use informative prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.20 Class conditional densities for binary data\n",
    "Consider a generative classifier for C classes with class conditional density p(x|y) and uniform class prior\n",
    "p(y). Suppose all the D features are binary, $x_j ∈ {0, 1}$. If we assume all the features are conditionally\n",
    "independent (the naive Bayes assumption), we can write<br>\n",
    "$\\quad\\begin{aligned} p(x|y = c) = \\prod^{D}_{j=1}Ber(x_j |θ_{jc}) \\end{aligned}$ <span style=\"float:right\"> (3.108) </span><br>\n",
    "This requires DC parameters.<br>\n",
    "a. Now consider a different model, which we will call the “full” model, in which all the features are fully\n",
    "dependent (i.e., we make no factorization assumptions). How might we represent p(x|y = c) in this\n",
    "case? How many parameters are needed to represent p(x|y = c)?<br>\n",
    "b. Assume the number of features D is fixed. Let there be N training cases. If the sample size N is very\n",
    "small, which model (naive Bayes or full) is likely to give lower test set error, and why?<br>\n",
    "c. If the sample size N is very large, which model (naive Bayes or full) is likely to give lower test set error,\n",
    "and why?<br>\n",
    "d. What is the computational complexity of fitting the full and naive Bayes models as a function of N\n",
    "and D? Use big-Oh notation. (Fitting the model here means computing the MLE or MAP parameter\n",
    "estimates. You may assume you can convert a D-bit vector to an array index in O(D) time.)<br>\n",
    "e. What is the computational complexity of applying the full and naive Bayes models at test time to a\n",
    "single test case?<br>\n",
    "f. Suppose the test case has missing data. Let $x_v$ be the visible features of size v, and $x_h$ be the hidden\n",
    "(missing) features of size h, where v + h = D. What is the computational complexity of computing\n",
    "$p(y|x_v, \\hat{θ})$ for the full and naive Bayes models, as a function of v and h?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### answer\n",
    "---\n",
    "### a\n",
    "$\\begin{aligned} p(x|y = c) = p(x_1 = 0/1, ..., x_D = 0/1|y) \\end{aligned}$<br>\n",
    "$\\begin{aligned} \\sum p(x|y = c) = 1 \\to N_{parameters} = C \\times (2^D - 1) \\end{aligned}$\n",
    "### b\n",
    "naive, because complex model needs more data, otherwise it'll be easy to overfit\n",
    "### c\n",
    "full, because if data is large enough, simple model will probably be underfit\n",
    "### d\n",
    "According to algorithm 3.1, O(ND)<br>\n",
    "According to 3.48, O(ND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e\n",
    "naive, O(CD)<br>\n",
    "full, O(max(D, C))\n",
    "### f\n",
    "naive, O(CV) to compute p(x)<br>\n",
    "full, if first marginlize those h features, then it needs O(max(V, C)) to compute p(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.21 Mutual information for naive Bayes classifiers with binary features\n",
    "Derive Equation 3.76."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### answer\n",
    "---\n",
    "$\\quad\\begin{aligned} I(X, Y) = \\sum_{x_j}\\sum_y p(x_j, y) log \\frac{p(x_j, y)}{p(x_j)p(y)} \\end{aligned}$ <span style=\"float:right\"> (3.75) </span><br>\n",
    "$\\quad\\begin{aligned} I_j = \\sum_c \\left[ \\theta_{jc}\\pi_clog\\frac{\\theta_{jc}}{\\theta_j} + (1 - \\theta_{jc})\\pi_clog\\frac{1-\\theta_{jc}}{1-\\theta_j} \\right] \\end{aligned}$ <span style=\"float:right\"> (3.76) </span><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{aligned} I_j = \\sum_c \\sum_{x_j} p(x_j|y=c)p(y=c) log \\frac{p(x_j|y=c)}{p(x_j)} = \\end{aligned}$<br>$\\begin{aligned} \\sum_c \\left(p(x_j = 1|y=c)p(y=c) log \\frac{p(x_j = 1|y=c)}{p(x_j = 1)} + p(x_j = 0|y=c)p(y=c) log \\frac{p(x_j = 0|y=c)}{p(x_j = 0)}\\right) \\to (3.76) \\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.22 Fitting a naive bayes spam filter by hand\n",
    "(Source: Daphne Koller.). Consider a Naive Bayes model (multivariate Bernoulli version) for spam classification\n",
    "with the vocabulary V=\"secret\", \"offer\", \"low\", \"price\", \"valued\", \"customer\", \"today\", \"dollar\", \"million\",\n",
    "\"sports\", \"is\", \"for\", \"play\", \"healthy\", \"pizza\". We have the following example spam messages \"million dollar\n",
    "offer\", \"secret offer today\", \"secret is secret\" and normal messages, \"low price for valued customer\", \"play\n",
    "secret sports today\", \"sports is healthy\", \"low price pizza\". Give the MLEs for the following parameters:\n",
    "$θ_{spam}, θ_{secret|spam}, θ_{secret|non-spam}, θ_{sports|non-spam}, θ_{dollar|spam}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### answer\n",
    "---\n",
    "$\\begin{aligned} \\theta_{spam} = \\frac{3}{3+4} \\end{aligned}$<br>\n",
    "$\\begin{aligned} \\theta_{secret|spam} = \\frac{2}{3} \\end{aligned}$<br>\n",
    "$\\begin{aligned} \\theta_{secret|non-spam} = \\frac{1}{4} \\end{aligned}$<br>\n",
    "$\\begin{aligned} \\theta_{sports|non-spam} = \\frac{2}{4} \\end{aligned}$<br>\n",
    "$\\begin{aligned} \\theta_{dollar|spam} = \\frac{1}{3} \\end{aligned}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
