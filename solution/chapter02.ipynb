{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1 Probabilities are sensitive to the form of the question that was used to generate the answer\n",
    "\n",
    "(Source: Minka.) My neighbor has two children. Assuming that the gender of a child is like a coin flip,\n",
    "it is most likely, a priori, that my neighbor has one boy and one girl, with probability 1/2. The other\n",
    "possibilities—two boys or two girls—have probabilities 1/4 and 1/4.\n",
    "\n",
    "a. Suppose I ask him whether he has any boys, and he says yes. What is the probability that one child is\n",
    "a girl?<br>\n",
    "b. Suppose instead that I happen to see one of his children run by, and it is a boy. What is the probability\n",
    "that the other child is a girl?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### answer\n",
    "---\n",
    "### a\n",
    "p(girl > 0 | boy > 0) = p(girl > 0, boy > 0) / p(boy > 0) = 1/2 / (1/2 + 1/4) = 2/3\n",
    "### b\n",
    "p(other girl = 1) = 1/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2 Legal reasoning\n",
    "\n",
    "(Source: Peter Lee.) Suppose a crime has been committed. Blood is found at the scene for which there is\n",
    "no innocent explanation. It is of a type which is present in 1% of the population.\n",
    "\n",
    "a. The prosecutor claims: “There is a 1% chance that the defendant would have the crime blood type if he\n",
    "were innocent. Thus there is a 99% chance that he guilty”. This is known as the prosecutor’s fallacy.<br>\n",
    "What is wrong with this argument?<br>\n",
    "b. The defender claims: “The crime occurred in a city of 800,000 people. The blood type would be\n",
    "found in approximately 8000 people. The evidence has provided a probability of just 1 in 8000 that\n",
    "the defendant is guilty, and thus has no relevance.” This is known as the defender’s fallacy. <br>\n",
    "What iswrong with this argument?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### answer\n",
    "---\n",
    "it's meaningless to use low chance event alone, should use it with other evidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3 Variance of a sum\n",
    "\n",
    "Show that the variance of a sum is var [X + Y ] = var [X] + var [Y ] + 2cov [X, Y ] , where cov [X, Y ]\n",
    "is the covariance between X and Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### answer\n",
    "---\n",
    "$var[X + Y] = E[(X - \\bar{X} + Y - \\bar{Y})^2] = var[X] + var[Y] + 2*E[(X - \\bar{X})(Y - \\bar{Y})] = var[X] + var[Y] + 2cov[X, Y]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.4 Bayes rule for medical diagnosis\n",
    "\n",
    "(Source: Koller.) After your yearly checkup, the doctor has bad news and good news. The bad news is that\n",
    "you tested positive for a serious disease, and that the test is 99% accurate (i.e., the probability of testing\n",
    "positive given that you have the disease is 0.99, as is the probability of tetsing negative given that you don’t\n",
    "have the disease). The good news is that this is a rare disease, striking only one in 10,000 people. What are\n",
    "the chances that you actually have the disease? (Show your calculations as well as giving the final result.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### answer\n",
    "---\n",
    "x = 1 means you have the disease<br>\n",
    "y = 1 means test positive<br>\n",
    "p(y=1 | x=1) = 0.99<br>\n",
    "p(y=0 | x=0) = 0.99<br>\n",
    "p(x=1 | y=1) = p(x=1, y=1)/p(y=1) = p(y=1 | x=1)p(x=1)/(p(x=1)p(y=1 | x=1) + p(x=0)p(y=1 | x=0)<br>\n",
    "= 0.99*0.0001/(0.0001*0.99 + 0.9999*0.01) = 0.99 / (0.99 + 99.99) = 0.0098<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.5 The Monty Hall problem\n",
    "\n",
    "(Source: Mackay.) On a game show, a contestant is told the rules as follows:<br>\n",
    "There are three doors, labelled 1, 2, 3. A single prize has been hidden behind one of them. You\n",
    "get to select one door. Initially your chosen door will not be opened. Instead, the gameshow host\n",
    "will open one of the other two doors, and he will do so in such a way as not to reveal the prize. For\n",
    "example, if you first choose door 1, he will then open one of doors 2 and 3, and it is guaranteed\n",
    "that he will choose which one to open so that the prize will not be revealed.<br>\n",
    "At this point, you will be given a fresh choice of door: you can either stick with your first choice,\n",
    "or you can switch to the other closed door. All the doors will then be opened and you will receive\n",
    "whatever is behind your final choice of door.<br>\n",
    "Imagine that the contestant chooses door 1 first; then the gameshow host opens door 3, revealing nothing\n",
    "behind the door, as promised. Should the contestant (a) stick with door 1, or (b) switch to door 2, or (c)\n",
    "does it make no difference? You may assume that initially, the prize is equally likely to be behind any of\n",
    "the 3 doors. <br>\n",
    "Hint: use Bayes rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### answer\n",
    "---\n",
    "p(Door1 = 1 | Door1 is chosen) = 1/3<br>\n",
    "p(Door2 = 1 or Door3 = 1 | Door1 is chosen) = 2/3<br>\n",
    "p(Door3 = 1 | Door1 is chosen, Door2 is opened) = <br>\n",
    "P(Door3 = 1, Door2 is opened | Door1 is chosen) / p(Door2 is opened | Door1 is chosen)  = <br>\n",
    "2/3*1/2 / (1/3*1/2 + 2/3*1/2) = 2 / 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.6 Conditional independence\n",
    "\n",
    "(Source: Koller.)<br>\n",
    "a. Let H ∈ {1,...,K} be a discrete random variable, and let $e_1$ and $e_2$ be the observed values of two\n",
    "other random variables $E_1$ and $E_2$. Suppose we wish to calculate the vector<br>\n",
    "$$\\overrightarrow{P} (H|e_1, e_2)=(P(H = 1|e_1, e_2),...,P(H = K|e_1, e_2))$$\n",
    "Which of the following sets of numbers are sufficient for the calculation?<br>\n",
    "i. P($e_1$, $e_2$), P(H), P($e_1$|H), P($e_2$|H)<br>\n",
    "ii. P($e_1$, $e_2$), P(H), P($e_1$, $e_2$|H)<br>\n",
    "iii. P($e_1$|H), P($e_2$|H), P(H)<br>\n",
    "b. Now suppose we now assume $E_1$ ⊥ $E_2$|H (i.e., $E_1$ and $E_2$ are conditionally independent given H).<br>\n",
    "Which of the above 3 sets are sufficent now?<br>\n",
    "Show your calculations as well as giving the final result. <br>\n",
    "Hint: use Bayes rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### answer\n",
    "---\n",
    "### a \n",
    "P(H|e1,e2) = P(H, e1,e2)/P(e1,e2) = P(e1,e2|H)P(H)/P(e1,e2)<br>\n",
    "ii\n",
    "### b \n",
    "P(e1,e2|H) = P(e1|H)P(e2|H)<br>\n",
    "i, ii"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.7 Pairwise independence does not imply mutual independence\n",
    "\n",
    "We say that two random variables are pairwise independent if<br>\n",
    "$\\quad p(X_2|X_1) = p(X_2)$ <span style=\"float:right\"> (2.125) </span><br>\n",
    "and hence<br>\n",
    "$\\quad p(X_2, X_1) = p(X_1)p(X_2|X_1) = p(X_1)p(X_2)$ <span style=\"float:right\"> (2.126) </span><br>\n",
    "We say that n random variables are mutually independent if <br>\n",
    "$\\quad p(X_i|X_S) = p(X_i)\\quad ∀S ⊆ {1,...,n}$ <span style=\"float:right\"> (2.127) </span><br>\n",
    "and hence<br>\n",
    "$\\quad p(X_{1:n}) = \\sum_{i=1}^{n} p(X_i)$ <span style=\"float:right\"> (2.128) </span><br>\n",
    "Show that pairwise independence between all pairs of variables does not necessarily imply mutual independence.\n",
    "It suffices to give a counter example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### answer\n",
    "---\n",
    "https://en.wikipedia.org/wiki/Pairwise_independence<br>\n",
    "Suppose X, Y are two independent tosses of a fair coin, where we designate 1 for heads and 0 for tails.\n",
    "Let the third random variable Z be equal to 1 if exactly one of those coin tosses resulted in 'heads', and\n",
    "0 otherwise.\n",
    "\n",
    "P(X) = 1/2<br>\n",
    "P(Y) = 1/2<br>\n",
    "P(Z) = 1/2<br>\n",
    "P(X, Y) = 1/4<br>\n",
    "P(X, Z) = 1/4<br>\n",
    "P(Y, Z) = 1/4<br>\n",
    "P(X, Y, Z) = P(Z|X, Y)P(X, Y) = P(Z|X, Y)P(X)P(Y)<br>\n",
    "P(Z|X,Y) = 0 or 1 != P(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.8 Conditional independence iff joint factorizes\n",
    "In the text we said X ⊥ Y |Z iff<br>\n",
    "$\\quad p(x, y|z) = p(x|z)p(y|z)$ <span style=\"float:right\"> (2.129) </span><br>\n",
    "for all x, y, z such that p(z) > 0. Now prove the following alternative definition: X ⊥ Y |Z iff there exist\n",
    "function g and h such that<br>\n",
    "$\\quad p(x, y|z) = g(x, z)h(y, z)$ <span style=\"float:right\"> (2.130) </span><br>\n",
    "for all x, y, z such that p(z) > 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### answer\n",
    "---\n",
    "$p(x, y | z) = g(x, z)h(y, z)$ <br>\n",
    "$p(x|z) = \\int g(x, z)h(y, z)dy = A*g(x, z)$ <br>\n",
    "$p(y|z) = \\int g(x, z)h(y, z)dx = B*h(y, z)$ <br>\n",
    "$1 = \\int \\int g(x, z)h(y, z)dxdz = A*B$ <br>\n",
    "$ p(x, y | z) = g(x, z)h(y, z) = A*g(x, z)*1/A*h(y, z) = p(x|z)*p(y|z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.9 Conditional independence\n",
    "(Source: Koller.) Are the following properties true? Prove or disprove. Note that we are not restricting\n",
    "attention to distributions that can be represented by a graphical model.<br>\n",
    "a. True or false? (X ⊥ W|Z, Y ) ∧ (X ⊥ Y |Z) ⇒ (X ⊥ Y,W|Z)<br>\n",
    "b. True or false? (X ⊥ Y |Z) ∧ (X ⊥ Y |W) ⇒ (X ⊥ Y |Z,W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### answer\n",
    "--- \n",
    "### a. \n",
    "true <br>\n",
    "$p(x, y, w | z) = p(x, w | z, y)*p(y | z) = p(x | z, y)p(w | z, y)p(y | z)\n",
    "= p(x | z)*p(y | z)*p(w | z, y) = p(x | z)*p(y, w | z)$\n",
    "### b. \n",
    "false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.10 Deriving the inverse gamma density\n",
    "Let X ∼ Ga(a, b), i.e.<br>\n",
    "$\\quad Ga(x|a, b) = \\frac{b^a}{Γ(a)}x^{a−1}e^{−xb}$ <span style=\"float:right\"> (2.131) </span><br>\n",
    "Let Y = 1/X. Show that Y ∼ IG(a, b), i.e.,<br>\n",
    "$\\quad IG(x|shape = a,scale = b) = \\frac{b^a}{Γ(a)}x^{−(a+1)}e^{−b/x}$ <span style=\"float:right\"> (2.132) </span><br>\n",
    "Hint: use the change of variables formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### answer\n",
    "--- \n",
    "$dx/dy=1/y^2$<br>\n",
    "$IG(x|shape=a, scale=b) = Ga(1/x)/x^2 = \\frac{b^a}{Γ(a)} * x^{(-a-1)} * e^{-b/x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.11 Normalization constant for a 1D Gaussian\n",
    "The normalization constant for a zero-mean Gaussian is given by<br>\n",
    "$\\quad Z =\\int^{b}_{a}exp(− \\frac{x^2}{2σ^2})dx $ <span style=\"float:right\"> (2.133) </span><br>\n",
    "where $a = −\\infty$ and $b = \\infty$. To compute this, consider its square<br>\n",
    "$\\quad Z^2 = \\int^{b}_{a}\\int^{b}_{a}exp (−\\frac{x^2 + y^2}{2σ^2}) dxdy  $ <span style=\"float:right\"> (2.134) </span><br>\n",
    "Let us change variables from cartesian (x, y) to polar (r, θ) using x = r cos θ and y = r sin θ. Since\n",
    "dxdy = rdrdθ, and cos2θ + sin2 θ = 1, we have<br>\n",
    "$\\quad Z^2 = \\int^{2π}_{0}\\int^{∞}_{0} r exp (− \\frac{r^2}{2σ^2})drdθ  $ <span style=\"float:right\"> (2.135) </span><br>\n",
    "Evaluate this integral and hence show $Z = σ\\sqrt{(2π)}$. \n",
    "Hint 1: separate the integral into a product of two terms, the first of which (involving dθ) is constant, so is easy. \n",
    "Hint 2: if $u = e^{−r^2/2σ^2}$ then $du/dr = − \\frac{1}{σ^2} re^{−r^2/2σ^2}$ , so the second integral is also easy (since $\\int u'(r)dr = u(r)$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### answer\n",
    "--- \n",
    "$Z^2 = \\int_{0}{2π}\\int_{0}^{∞} r*exp(-r^2/(2*σ^2))drdθ$<br>\n",
    "$= 2π * \\int_{0}^{∞} r*exp(-r^2/(2*σ^2))dr$ <br>\n",
    "$= 2π * ∫(0, ∞)exp(-r^2/(2*σ^2))dr^2/2$ <br>\n",
    "$= 2π * σ^2 * ∫(0, ∞)exp(-r^2/(2*σ^2))dr^2/(2*σ^2)$ <br>\n",
    "$ = 2π * σ^2 * ∫(0, ∞)exp(-z)dz$ <br>\n",
    "$ = 2π * σ^2$ <br>\n",
    "$ Z = σ * \\sqrt{2π}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.12 Expressing mutual information in terms of entropies\n",
    "Show that<br>\n",
    "$\\quad I(X, Y ) = H(X) − H(X|Y ) = H(Y ) − H(Y |X)$ <span style=\"float:right\"> (2.136) </span><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### answer\n",
    "--- \n",
    "$I(X, Y) = ∑_x∑_y p(x, y)log(\\frac{p(x, y)}{p(x)p(y)})$ <br>\n",
    "$ = ∑_x∑_y p(x, y)log(\\frac{p(x|y)}{p(x)})$ <br>\n",
    "$ = ∑_x∑_y p(x, y)logp(x|y) - ∑_x∑_y p(x, y)logp(x)$ <br>\n",
    "$ = ∑_y p(y)∑_x p(x|y)logp(x|y) - ∑_x p(x)logp(x)$ <br>\n",
    "$ = H(X) - H(X|Y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.13 Mutual information for correlated normals\n",
    "(Source: (Cover and Thomas 1991, Q9.3).) Find the mutual information I(X1, X2) where X has a bivariate\n",
    "normal distribution:<br>\n",
    "$\\quad \\begin{pmatrix} X_1 \\\\ X_2 \\end{pmatrix} \\sim N \\left( 0, \\begin{pmatrix}σ^2 & ρσ^2 \\\\ ρσ^2 & σ^2 \\end{pmatrix} \\right)$ <span style=\"float:right\"> (2.137) </span><br>\n",
    "Evaluate $I(X_1, X_2)$ for ρ = 1, ρ = 0 and ρ = −1 and comment. Hint: The (differential) entropy of a\n",
    "d-dimensional Gaussian is<br>\n",
    "$\\quad h(X) = \\frac{1}{2} log_2\\left[(2πe)^d det\\sum\\right] $ <span style=\"float:right\"> (2.138) </span><br>\n",
    "In the 1d case, this becomes <br>\n",
    "$\\quad h(X) = \\frac{1}{2} log_2\\left[2πeσ^2\\right] $ <span style=\"float:right\"> (2.139) </span><br>\n",
    "Hint: log(0) = ∞."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### answer\n",
    "---\n",
    "$I(X_1, X_2) = \\int_{x_1}\\int_{x_2} p(x_1, x_2)log\\frac{p(x_1, x_2)}{p(x1)*p(x2)}dx1dx2$<br>\n",
    "$= \\int_{x_1}\\int_{x_2} p(x_1, x_2)logp(x_1, x_2)dx_1dx_2 - \\int_{x_1}\\int_{x_2} p(x_1, x_2)logp(x_1)dx_1dx_2 - \\int_{x_1}\\int_{x_2} p(x_1, x_2)logp(x_2)dx_1dx_2$<br>\n",
    "$=  \\int_{x_1}\\int_{x_2} p(x_1, x_2)logp(x_1, x_2)dx_1dx_2 - \\int_{x_1} p(x_1)logp(x_1)dx_1 - \\int_{x_2} p(x_2)logp(x_2)dx_2$<br>\n",
    "$= H(X_1) + H(X_2) - H(X_1, X_2)$<br>\n",
    "$= log_2(2πeσ^2) - 1/2*log_2((2πe)^2det∑)$<br>\n",
    "$= log_2(2πeσ^2) - 1/2*log_2((2πe)^2*(1-ρ^2)*σ^4)$<br>\n",
    "$= log_2(1/\\sqrt{1-ρ^2})$<br>\n",
    "$= -1/2*log_2(1-ρ^2)$<br>\n",
    "<br>\n",
    "$ρ = 1$ <br>\n",
    "$I(X1, X2) = ∞$ <br>\n",
    "\n",
    "$ρ = 0$<br>\n",
    "$I(X1, X2) = 0$<br>\n",
    "\n",
    "$ρ = -1$<br>\n",
    "$I(X_1, X_2) = ∞$<br>\n",
    "\n",
    "the more correlated, the bigger mutual information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.14 A measure of correlation (normalized mutual information)\n",
    "(Source: (Cover and Thomas 1991, Q2.20).) Let X and Y be discrete random variables which are identically\n",
    "distributed (so H(X) = H(Y)) but not necessarily independent. Define<br>\n",
    "$\\quad r = 1 − H(Y |X)H(X) $ <span style=\"float:right\"> (2.140) </span><br>\n",
    "a. Show r = I(X,Y )/H(X) <br>\n",
    "b. Show 0 ≤ r ≤ 1 <br>\n",
    "c. When is r = 0? <br>\n",
    "d. When is r = 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### answer\n",
    "---\n",
    "### a.\n",
    "I(X, Y)/H(X) = (H(Y) - H(Y|X))/H(X) = 1 - H(Y|X)/H(X) = r\n",
    "### b.\n",
    "$I(X, Y) =  ∑_x∑_y p(x, y)log(\\frac{p(x, y)}{p(x)p(y)})$<br>\n",
    "$ =  ∑_x∑_y p(x, y)-log(\\frac{p(x)p(y)}{p(x, y)})$<br>\n",
    "$ >= -log(∑_x∑_y p(x)p(y) = -log1 = 0$<br>\n",
    "I(X, Y) = H(X) - H(X|Y)<br>\n",
    "H(X) >= H(X|Y)<br>\n",
    "for this problem<br>\n",
    "I(X, Y) >= 0<br>\n",
    "H(X) >= 0<br>\n",
    "r = I(X, Y)/H(X) >= 0<br>\n",
    "H(Y|X) >= 0<br>\n",
    "H(Y|X)/H(X) >= 0<br>\n",
    "r = 1 - H(Y|X)/H(X) <= 1\n",
    "### c.\n",
    "r = 0 when H(Y|X) = H(Y), in other words X and Y are independent\n",
    "### d.\n",
    "r = 1 when H(Y|X) = 0, in other words Y is completely determined by X and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.15 MLE minimizes KL divergence to the empirical distribution\n",
    "Let $p_{emp}(x)$ be the empirical distribution, and let $q(x|θ)$ be some model. Show that $argmin_q KL (p_{emp}||q)$\n",
    "is obtained by $q(x) = q(x; \\hat{θ})$, where $\\hat{θ}$ is the MLE. Hint: use non-negativity of the KL divergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### answer\n",
    "---\n",
    "$KL(p_{emp}||q) >= 0$ <br>\n",
    "$KL(p_{emp}||p_{emp}) = 0$ <br>\n",
    "$argmin_q KL(p_{emp}||q) \\to q = p_{emp}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.16 Mean, mode, variance for the beta distribution\n",
    "Suppose $θ ∼ Beta(a, b)$. Derive the mean, mode and variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### answer\n",
    "---\n",
    "$Beta(a, b) = \\frac{1}{B(a, b)}x^{a-1}*(1-x)^{b-1}$<br>\n",
    "### mean\n",
    "$mean = \\int_x x*Beta(a, b)dx = \\frac{1}{B(a, b)}\\int_x x^a*(1-x)^{b-1}dx$<br>\n",
    "$= \\frac{B(a+1, b)}{B(a, b)} = \\frac{a}{a+b}$\n",
    "### mode\n",
    "$ \\frac{a-1}{x} = \\frac{b-1}{1-x} \\to x = \\frac{a-1}{a+b-2}$\n",
    "### variance\n",
    "$ var[x] = E[X^2] - E[X]^2 = \\frac{(a+1)a}{(a+b+1)(a+b)} - \\frac{a^2}{(a+b)^2} = \\frac{ab}{(a+b+1)(a+b)^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.17 Expected value of the minimum\n",
    "Suppose X, Y are two points sampled independently and uniformly at random from the interval [0, 1].\n",
    "What is the expected location of the left most point?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### answer\n",
    "---\n",
    "$E(min(x, y)) = \\int_{0}^{1} p(x) (\\int_{0}^{x} y*p(y)dy + \\int_{x}^{1} x*p(y)dy) dx$ <br>\n",
    "$ = \\int_{0}^{1} p(x) (x^2/2 + x*(1-x))dx = 1/6 + 1/2 - 1/3 = 1/3$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
